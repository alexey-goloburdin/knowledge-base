Здоров котаны, вчера вышло видео про оптимизацию вложенных циклов с примером на Python, которое вызвало бурное обсуждение в комментариях и я решил снять дополнение ко вчерашнему видосу.

Да, выводы, которые звучали во вчерашнем видосе, оказались поспешными и не всегда правильными, как мы сейчас убедимся. Однако вот сейчас мы с вами во всём разберёмся и сделаем прррравильные выводы!

/ ну, скорее всего...

Коротенько напомню, об чём речь. Вывод в видосе: когда у вас есть вложенные циклы, для эффективности имеет смысл цикл меньшего размера ставить наружным, а цикл большего размера ставить внутренним.

Для подтверждения сего поступала демонстрировался совершенно великолепный код на питоняке:

```python
import random
import timeit

ROWS = 5
COLUMNS = 100
TIMEIT_ITERATIONS = 100_000

table = tuple(tuple(random.randint(1000, 10_000)
                    for _ in range(COLUMNS))
              for _ in range(ROWS))

def big_outer_loop():
    overall_sum = 0
    for column in range(COLUMNS):
        for row in range(ROWS):
            overall_sum += table[row][column]

def small_outer_loop():
    overall_sum = 0
    for row in range(ROWS):
        for column in range(COLUMNS):
            overall_sum += table[row][column]


small_time = round(timeit.timeit(small_outer_loop, number=TIMEIT_ITERATIONS), 2)
big_outer_loop_time = round(timeit.timeit(big_outer_loop, number=TIMEIT_ITERATIONS), 2)

delta = round(100*(big_outer_loop_time - small_outer_loop_time) / big_outer_loop_time)
print(f"{delta}%")
```

Здесь есть таблице на 5 строк и 500 колонок и мы проходим эту таблицу либо сначала по колонкам, потом по строкам, либо сначала по строкам, потом по колонкам. И в моём тесте получалось, что эффективнее внешний цикл ставить тот, в котором меньше итераций, потому что это отрабатывало на 27% быстрее.

В комментариях написали, что это у тебя оно так работает, потому что ты проходишь по структуре и эффективнее проходить по данным, которые лежат на диске или в памяти последовательно, ибо кэши процессора и так далее. Конечно, это так, однако в данном случае это не особо влияет на результаты, давайте в этом убедимся, вообще выбросив таблицу.

Также звучали призывы вынести за рамки функций `range`, потому что создание `range` во внутреннем цикле тоже может оказывать своё влияние. Давайте избавимся от `range` и вообще избавимся от неявных счетчиков циклов, потому что то, как я считал итерации в прошлом видео, тоже многим не понравилось, давайте всю подкапотную машинерию просто вынесем наружу. А то людям кажется, что циклы `for` бесплатны, это не так, конечно, инициализация счетчиков, их инкремент и проверка вполне себе занимают время.

```python
import sys
import timeit

BIG = int(sys.argv[1])
SMALL = int(sys.argv[2])
TIMEIT_ITERATIONS = int(sys.argv[3])

def func1():
    i = 0
    overall_sum = 0
    while i < BIG:
        j = 0
        while j < SMALL:
            overall_sum += 1
            j += 1
        i += 1
    return overall_sum

def func2():
    i = 0
    overall_sum = 0
    while i < SMALL:
        j = 0
        while j < BIG:
            overall_sum += 1
            j += 1
        i += 1
    return overall_sum

print(f"{func1()=}, {func2()=}")

big = timeit.timeit(func1, number=TIMEIT_ITERATIONS)
small = timeit.timeit(func2, number=TIMEIT_ITERATIONS)
print(f"\ndelta is {round(100*(big-small)/big)}%")
```

Два совершенно идентичных цикла. Никаких структур данных не создаётся и не обходится, никаких `range` не используется. Дельта получается 17%.



---

OLD text




Тем самым всё равно получаем, что внешний меньший цикл в данном примере эффективнее на 26%. Из чего можно сделать предварительный вывод, что дело тут не в удобном последовательном или неудобном непоследовательном чтении кортежа.

Ещё одно пожелание было избавиться от `range` внутри тестируемых функций, давайте мол создадим генераторы один раз вместо повторного создания, потому что это тоже может вносить дополнительный расколбас. Логика в этом суждении есть, мы хотим тестировать размер циклов, а не как ведут себя `range` при разных вызовах, давайте так сделаем.

```python
import timeit

ROWS = range(5)
COLUMNS = range(100)
TIMEIT_ITERATIONS = 100_000

def big_outer_loop():
    overall_sum = 0
    for column in COLUMNS:
        for row in ROWS:
            overall_sum += row + column + 1

def small_outer_loop():
    overall_sum = 0
    for row in ROWS:
        for column in COLUMNS:
            overall_sum += row + column + 1

small_outer_loop_time = timeit.timeit(small_outer_loop, number=TIMEIT_ITERATIONS)
big_outer_loop_time = timeit.timeit(big_outer_loop, number=TIMEIT_ITERATIONS)

delta = round(100*(big_outer_loop_time - small_outer_loop_time) / big_outer_loop_time)
print(f"{delta}%")
```

17%. Процент уменьшился, но это всё равно не статистическая погрешность, вариант с меньшим внешним циклом пока ощутимо выигрывает. Давайте уменьшим разбег между размером внешнего и внутреннего цикла:

```python
ROWS = range(50)
COLUMNS = range(100)
TIMEIT_ITERATIONS = 10_000
```

Получаем разницу в районе нуля процентов, уже статистически похожую на погрешность. То есть если внешний и внутренний цикл отличаются не очень сильно, то пофик какой цикл будет внешним. Окей, давайте наоборот усилим разницу:

```python
ROWS = range(5)
COLUMNS = range(500)
TIMEIT_ITERATIONS = 10_000
```

Аналогично, в районе нуля. Разница есть, но несущественная и колеблется. А так?

```python
ROWS = range(50)
COLUMNS = range(500)
TIMEIT_ITERATIONS = 10_000
```

Получим отрицательное значение -10%, то есть наша так называемая оптимизация наоборот сделала хуже. Уопляля!

То есть что получается — на значениях 5 — 100 у нас есть оптимизация на 27%, на значениях 5 — 500 наша оптимизация не даёт видимых результатов, на значениях 50 — 500 даёт отрицательный эффект вообще.

Какой вывод отсюда можно сделать? Что это сложно назвать оптимизацией, которая всегда работает и которую можно взять за правило. Компиляторы и интерпретаторы сейчас достаточно умные, чтобы оптимизировать какие-то вещи даже без нашего ведома и общий вывод тут как не странно классический — надо тестировать конкретный кусок кода и при необходимости оптимизировать именно его. То, что нам может казаться оптимизацией, может как быть ей в каких-то случаях, так и не оказывать никакого осязаемого эффекта, так и вовсе иметь негативный эффект. Всегда надо смотреть конкретный кейс.

Такие дела:)

И ещё одно. В комментариях были большие баталии касательно количества итераций. Мой поинт в том, что если внешний цикл имеет 100 итераций, а внутренний 5, то общее количество итераций 600. Если циклы меняем местами, то 505. В комментариях некоторые люди оппонировали, что итераций всегда 500, 5 умножить на 100.

С этим согласиться я не готов, давайте посмотрим такой пример.

```python
def my_range(limit):
    counter = 0
    while counter < limit:
        yield 1
        counter += 1
    my_range.called += counter

my_range.called = 0

for _ in my_range(100):
    for _ in my_range(5):
        pass

print(my_range.called)  # 600

my_range.called = 0

for _ in my_range(5):
    for _ in my_range(100):
        pass

print(my_range.called)  # 505
```

Я тут накидал свой вариант `my_range`, который считает количество своих вызовов и сохраняет это количество как атрибут себя с названием `called`. Это неэффективный подход

```
100 5 100 — 17-19%
1000 5 100 — примерно аналогично
1000 50 100 — -5-7%, пошла деградация
```


[[Сценарий]]